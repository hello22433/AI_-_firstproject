
#!/usr/bin/env python
# coding: utf-8

# In[14]:
from emotion_economics import get_stock_buy_recommendation, LSTM_pre
import streamlit as st
import requests
from bs4 import BeautifulSoup
from transformers import pipeline

from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
import time
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support import expected_conditions as EC

import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np


# In[31]:

# Set up the Streamlit page
st.set_page_config(page_title="ì£¼ì‹ íˆ¬ì ì˜ê²¬ ë¶„ì„ê¸°", page_icon="ğŸ“ˆ")

# Add title and description
st.title("ğŸ“ˆ ì£¼ì‹ íˆ¬ì ì˜ê²¬ ë¶„ì„ê¸°")
st.markdown("""
ì´ ì•±ì€ ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ì„ ë¶„ì„í•˜ì—¬ ì£¼ì‹ íˆ¬ì ì˜ê²¬ì„ ì œê³µí•©ë‹ˆë‹¤.
""")

# Create input field
stock_name = st.text_input("í‹°ì»¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:", placeholder="ì˜ˆ: 005930")
stock_name = str(stock_name)

# Create analyze button
if st.button("íˆ¬ìì˜ê²¬ ë¶„ì„"):
    if stock_name:
        try:
            # Get recommendation
            headline_results, buy_recommendation = get_stock_buy_recommendation(stock_name)

            # Display results
            st.subheader(f"{stock_name}ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼")

            # Display buy recommendation in a highlighted box
            st.info(buy_recommendation)

            # Display headlines in an expandable section
            with st.expander("ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìƒì„¸ ë¶„ì„ ë³´ê¸°"):
                for headline, emoji in headline_results:
                    st.write(f"{emoji} {headline}")
            
            LSTM_pre(stock_name)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    else:
        st.warning("ì¢…ëª©ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        
        



#ê²€ìƒ‰ ì•„ì´ì½˜ ì°¾ê¸°. [ì•„ì´ì½˜ì´ ì—†ì–´ í•„ìš”X]
# search_icon = driver.find_element(By.XPATH, '//*[@id="__next"]/header/div[1]/section/div[2]/div[1]/button/svg/path')
# print(search_icon.text)
options = Options()
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36")
options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
options.add_argument("--no-sandbox")  # ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ í•„ìˆ˜ì¸ ê²½ìš°ê°€ ë§ìŒ
options.add_argument("--disable-dev-shm-usage")  # ë¦¬ëˆ…ìŠ¤ í™˜ê²½ì—ì„œ í•„ìˆ˜ì¸ ê²½ìš°ê°€ ë§ìŒ
options.add_argument("--disable-gpu")  # GPU ê°€ì† ë¹„í™œì„±í™” (ì¼ë¶€ í™˜ê²½ì—ì„œ í•„ìš”)
options.add_argument("--window-size=1920,1080")  # ë¸Œë¼ìš°ì € ì°½ í¬ê¸° ì„¤ì •

driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)
url = "http://data.krx.co.kr/contents/MDC/MAIN/main/index.cmd"
driver.get(url)

# í˜ì´ì§€ ë¡œë”©ì‹œê°„
WebDriverWait(driver, 10).until(
    lambda driver: driver.execute_script('return document.readyState') == 'complete'
)
time.sleep(2) # í˜ì´ì§€ ë¡œë”©ì‹œê°„ ì•ˆì „ì¥ì¹˜


#ê²€ìƒ‰ ë²„íŠ¼ ë§‰ëŠ” ì°½ ë„ê¸° 
try : 
#     search_ad_button = WebDriverWait(driver, 20).until(
#     EC.visibility_of_element_located((By.XPATH, 'jsDetailLayer_MDCMAIN003"]/div[3]/div/button'))
# )
    search_ad_button = driver.find_element(By.XPATH, '//*[@id="jsDetailLayer_MDCMAIN003"]/div[3]/div/button')
    search_ad_button2 = driver.find_element(By.XPATH, '//*[@id="jsDetailLayer_MDCMAIN004"]/div[1]/button')
    search_ad_button2.click()
    search_ad_button.click()
except :
    pass
    
#ê²€ìƒ‰ ì¸í’‹ ì°¾ê¸°
# search_input = WebDriverWait(driver, 20).until(
#     EC.visibility_of_element_located((By.XPATH, '//*[@id="jsTotSch"]'))
# )
search_input = driver.find_element(By.XPATH, '//*[@id="jsTotSch"]')
#ê²€ìƒ‰ ë²„íŠ¼ ì°¾ê¸°
search_button = driver.find_element(By.XPATH, '//*[@id="jsTotSchBtn"]')
#ê²€ìƒ‰ í•­ëª© ë³€ê²½ì„ ìœ„í•´ì„œ ì—¬ê¸°ì„œ ì¢…ëª©ì´ë¦„ ë³€ê²½

#ì¢…ëª©ì´ë¦„
find_name = stock_name

#ê²€ìƒ‰ ì¸í’‹ ë„£ê¸°
def func_search_input(x) :
    search_input.send_keys(x)
    search_button.click()
func_search_input(find_name)
#ì •ë³´ ê°€ì ¸ì˜¤ê¸°

time.sleep(2)
#ë¶ˆëŸ¬ì˜¤ê¸° ì •ë³´ í† í°
find_stock = '//*[@id="isuInfoTitle"]/span'
find_incdec = '//*[@id="isuInfoTitle"]/span/dfn'
find_start = '//*[@id="isuInfoBind"]/table/tbody/tr[1]/td[1]'
find_high = '//*[@id="isuInfoBind"]/table/tbody/tr[2]/td[1]'
find_low = '//*[@id="isuInfoBind"]/table/tbody/tr[3]/td[1]'

#í† í° ë¶ˆëŸ¬ì˜¤ê¸°
sear_name = find_name
sear_stock = driver.find_element(By.XPATH, find_stock)
sear_incdec = driver.find_element(By.XPATH, find_incdec)
sear_start = driver.find_element(By.XPATH, find_start)
sear_high = driver.find_element(By.XPATH, find_high)
sear_low = driver.find_element(By.XPATH, find_low)


#ì£¼ê°€ê°€ ë°”ë¡œ ì¶”ì¶œì´ ì•ˆ ë˜ë¯€ë¡œ ì „ì²´strì—ì„œ ì¶”ì¶œ.
sear_stock_str = ''
for i in sear_stock.text :
    if i=='â–¼' or i=='â–²' or i=='(' or i=='-':
        break;
    sear_stock_str += i

#ìš”êµ¬ ì •ë³´ ì¶”ì¶œ
print("ì¢…ëª©ì´ë¦„ :", sear_name)
print("ì£¼ê°€ :", sear_stock_str)
print("ì£¼ê°€ì¦ê° :", sear_incdec.text)
print("ì‹œê°€ :", sear_start.text)
print("ê³ ê°€ :", sear_high.text)
print("ì €ê°€ :", sear_low.text)

with st.expander("ì£¼ê°€ ì •ë³´ ìì„¸íˆ ë³´ê¸°"): 
    st.write(f"ì¢…ëª©ì´ë¦„ : {sear_name}")
    st.write(f"ì£¼ê°€ : {sear_stock_str}")
    st.write(f"ì£¼ê°€ì¦ê° : {sear_incdec.text}")
    st.write(f"ì‹œê°€ : {sear_start.text}")
    st.write(f"ê³ ê°€ : {sear_high.text}")
    st.write(f"ì €ê°€ : {sear_low.text}")


#ì±—ì§€í”¼í‹°
from openai import OpenAI

myapikey = "ì—¬ê¸°ì— keyë¥¼ ì…ë ¥í•˜ì„¸ìš”"

client = OpenAI(api_key = myapikey)

response = client.chat.completions.create(
    model = 'gpt-4o-mini',
    messages = [
        {"role":"system", "content":"ë„ˆëŠ” ìŠ¤ë§ˆíŠ¸í•œ ê¸ˆìœµ ì •ë³´ ì œê³µ AI ì±—ë´‡ì´ì•¼."},
        {"role":"user", "content":f"ì‹¤ì‹œê°„ ê²€ìƒ‰ì„ ì´ìš©í•´ì„œ ì¢…ëª©ì´ë¦„ì¸ {sear_name}ê³¼, ì£¼ê°€ì¸ {sear_stock_str}ê³¼, ì£¼ê°€ì¦ê°ì¸ {sear_incdec.text}ê³¼, ì‹œê°€ì¸ {sear_start.text}ê³¼, ê³ ê°€ì¸ {sear_high.text}ì™€, ì €ê°€ì¸ {sear_low.text}ë¥¼ ì°¸ê³ í•´ì„œ {sear_name}ì˜ ì •ëŸ‰ì ì¸ ë¶„ì„ì„ í•´ì¤˜"}
    ],
    max_tokens=500
)

## ì±—ì§€í”¼í‹°ë¥¼ í™œìš©í•œ ì •ëŸ‰ì ì¸ ë¶„ì„
gpt_response = response.choices[0].message.content
with st.expander("ì£¼ê°€ ì •ë³´ ì •ëŸ‰ì ì¸ ë¶„ì„"): 
    st.write(gpt_response)


# ì›Œë“œí´ë¼ìš°ë“œì— ì‹œê°í™”í•  ì •ë³´(ë„¤ì´ë²„ ê²€ìƒ‰ ì •ë³´)

import requests
from bs4 import BeautifulSoup
import urllib.parse

def get_naver_autocomplete(query, limit):
    # ë„¤ì´ë²„ ìë™ì™„ì„± URL ìƒì„±
    base_url = "https://ac.search.naver.com/nx/ac"
    params = {
        'q': query,
        'con': '1',
        'frm': 'nv',
        'ans': '2',
        'r_format': 'json',
        'r_enc': 'UTF-8',
        'r_unicode': '0',
        'r_escape': '1',
        'st': '100'
    }
    
    response = requests.get(base_url, params=params)
    
    # ì‘ë‹µì´ ì„±ê³µì ì¼ ê²½ìš° JSON ë°ì´í„° íŒŒì‹±
    if response.status_code == 200:
        data = response.json()
        
        # ìë™ì™„ì„± í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        suggestions = [item[0] for item in data['items'][0]][:limit]
        return suggestions
    else:
        print("ìë™ì™„ì„± í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return []


# ì±— ì§€í”¼í‹° ì½”ë“œ ì¬í™œìš©í•˜ì—¬ ë„¤ì´ë²„ê²€ìƒ‰ì›Œë”©ê³¼ ì—°ê´€ëœ ì›Œë”© ì°¾ì•„ë‚´ê¸°
response = client.chat.completions.create(
    model = 'gpt-4o-mini',
    messages = [
        {"role":"system", "content":"ë„ˆëŠ” ìŠ¤ë§ˆíŠ¸í•œ ê¸ˆìœµ ì •ë³´ ì œê³µ AI ì±—ë´‡ì´ì•¼."},
        {"role":"user", "content":f"ì¢…ëª©ì´ë¦„ì¸ {sear_name}ì™€ ì—°ê´€ëœ ë‹¨ì–´ë§Œì„ 20ê°œë§Œ ë„ì–´ì“°ê¸°ë¡œ ì•Œë ¤ì¤˜"}
    ],
    max_tokens=500
)

gpt_response = response.choices[0].message.content

# ì±— ì§€í”¼í‹°ë¡œ ì°¾ì•„ë‚¸ ì—°ê´€ì›Œë”©ì„ ê°ê° ë„¤ì´ë²„ ì—°ê´€ ê²€ìƒ‰ì–´ ì•Œì•„ë‚´ê¸°
gtext = ''
samsung_related_word = gpt_response.split(' ')
for i in samsung_related_word : 
    query = i
    keywords = get_naver_autocomplete(query, limit=5)
    print("ìë™ì™„ì„± í‚¤ì›Œë“œ:", " ".join(keywords))
    gtext += " "
    gtext += " ".join(keywords)


# In[517]:

import nltk
from nltk.tokenize import wordpunct_tokenize
from nltk.tag import pos_tag

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


# In[519]:

import os #ìš´ìš©ì²´ì œ ê¸°ëŠ¥ ë„ì…
from collections import Counter #ë¹ˆë„ìˆ˜ ìœ„í•´ ë„ì…
import re #ì •ê·œì‹ ëª¨ë“ˆ
from nltk.corpus import stopwords #ë¶ˆìš©ì–´ ì‚¬ì „

# In[521]:

from konlpy.tag import Kkma
from konlpy.tag import Hannanum
from wordcloud import WordCloud, STOPWORDS # STOPWORDSì— ì˜ì–´ ë¶ˆìš©ì–´ë§Œ ë“¤ì–´ ìˆìœ¼ë¯€ë¡œ ì‚¬ì‹¤ í•„ìš” ì—†ìŒ
from PIL import Image

# In[523]:


# trump_mask = np.array(Image.open("real_black_trump.png"))
# trump_mask[0]


# In[524]:

import requests
from io import BytesIO

# ì´ë¯¸ì§€ë¥¼ ë„¤ì´ë²„ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
def get_image_from_naver(query, display=1):
    # ë„¤ì´ë²„ API í‚¤ (ë°œê¸‰ë°›ì€ IDì™€ Secretì„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”)
    client_id = "ì—¬ê¸°ì— id key"
    client_secret = "ì—¬ê¸°ì— pw key"
    url = "https://openapi.naver.com/v1/search/image"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }
    params = {
        "query": query,  # ê²€ìƒ‰í•  í‚¤ì›Œë“œ
        "display": display,  # ê°€ì ¸ì˜¬ ì´ë¯¸ì§€ ìˆ˜
        "start": 1,
        "sort": "sim"
    }

    response = requests.get(url, headers=headers, params=params)

    if response.status_code == 200:
        items = response.json().get('items')
        if items:
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°
            image_url = items[0].get("link")
            # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ë³€ìˆ˜ë¡œ ì €ì¥
            image_response = requests.get(image_url)
            if image_response.status_code == 200:
                image_data = Image.open(BytesIO(image_response.content))
                return image_data
            else:
                print("ì´ë¯¸ì§€ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        else:
            print("ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("API ìš”ì²­ ì‹¤íŒ¨:", response.status_code)

# ì´ë¯¸ì§€ë¥¼ ë„¤ì´ë²„ì—ì„œ ë¶ˆëŸ¬ì™€ ë³€ìˆ˜í™”
query = sear_name
image_data = get_image_from_naver(query)
st.write(image_data)

# ì´ë¯¸ì§€ í™•ì¸ í…ŒìŠ¤íŠ¸
# if image_data:
#     image_data.show()  # ì´ë¯¸ì§€ í™•ì¸


# In[525]:

# ì´ë¯¸ì§€ ì¶”ì¶œ ì¤‘ê°„ í™•ì¸
# plt.figure(figsize=(15,8))
# plt.imshow(image_data, cmap=plt.cm.gray, interpolation='bilinear')
# plt.axis('off')
# plt.show()


# In[526]:

import numpy as np
import re

# ì´ë¯¸ì§€ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_image_for_wordcloud(image):
    # imageê°€ numpy ë°°ì—´ í˜•ì‹ì´ë¼ë©´ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    
    # ì´ë¯¸ì§€ë¥¼ í‘ë°±ìœ¼ë¡œ ë³€í™˜
    image = image.convert("L")  # í‘ë°± ë³€í™˜
    
    # ì´ì§„í™” (í”½ì…€ ê°’ì´ 128ë³´ë‹¤ ì‘ìœ¼ë©´ 0, ì•„ë‹ˆë©´ 255ë¡œ ë³€í™˜)
    image = image.point(lambda x: 0 if x < 128 else 255, '1')

    # ì´ë¯¸ì§€ í¬ê¸° ì „í™˜
    # image = image.resize((800, 800))
    
    # ê²°ê³¼ ì´ë¯¸ì§€ í™•ì¸ì„ ìœ„í•œ ì¶œë ¥ (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)
    # image.show()  # ì´ ë¶€ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    mask = np.array(image)
    return mask

# ì´ë¯¸ì§€ë¥¼ ì›Œí´ë¼ìš°ë”©ì„ ìœ„í•´ ì „ì²˜ë¦¬í•œ í›„ ë³€ìˆ˜í™”
Mask = preprocess_image_for_wordcloud(image_data)


# ì „ì²˜ë¦¬ëœ ë§ˆìŠ¤í¬ í™•ì¸ (ì´ë¯¸ì§€ë¡œ ì¶œë ¥í•˜ê±°ë‚˜ ë§ˆìŠ¤í¬ ë°°ì—´ì„ í™•ì¸)
# print(Mask)



# In[527]:

def wordCloud_make(gtext) : 
    wc = WordCloud(
        background_color = 'white',
        max_words=2000,
        mask=Mask,
        contour_width=3,
        contour_color="steelblue",
        font_path = 'NanumGothicEco.ttf'
    )
    # text í‘œì¤€í™”
    import re
    gtext = re.sub(r'[^\w\s]', '', gtext) 

    # ë¶ˆìš©ì–´ ì§€ì •
    x_words = [
        'ì˜', 'ì—', 'ì…ë‹ˆë‹¤', 'ì´ë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ìˆ˜', 'ëœ»', 'ì˜ì–´ë¡œ', 'ë°', '400ì›', '400ì›ì€', '400ì›ìœ¼ë¡œ', '400ì›ì„'
    ]
    # textì—ì„œ ë¶ˆìš©ì–´ ê±¸ëŸ¬ë‚´ê¸°
    gtext = gtext.split(' ')
    bucket = ''
    for word in gtext :
        if not word in x_words:
            bucket += ' '
            bucket += word
            
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wc.generate(bucket)
    
    # word_list = list(wc.words_.keys())
    # word_list[0:10]
    
    # ì›Œë“œí´ë¼ìš°ë“œ ì¶œë ¥
    fig = plt.figure(figsize=(15,8))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.show()
    st.write(fig)

wordCloud_make(gtext)


# In[529]:


